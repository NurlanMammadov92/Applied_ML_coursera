{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "dataset = load_digits()\n",
    "\n",
    "X, y = dataset.data, dataset.target \n",
    "\n",
    "for class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n",
    "    print(class_name, class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a dataset with imbalanced binary classes:\n",
    "# Negative class (0) is 'not digit 1'\n",
    "# Positive class (1) is 'digit 1\n",
    "y_binary_imbalanced = y.copy()\n",
    "y_binary_imbalanced[y_binary_imbalanced != 1] = 0\n",
    "\n",
    "print('Original labels:\\t', y[1:30])\n",
    "print('New binary labels:\\t', y_binary_imbalanced[1:30])\n",
    "np.bincount(y_binary_imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel = 'rbf', C = 1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Negative class (0) is most frequent\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "# Therefore the dummy 'most_frequent' classifier always predicts class 0\n",
    "\n",
    "y_dummy_predictions = dummy_majority.predict(X_test)\n",
    "y_dummy_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_majority.score(X_test, y_test)\n",
    "svm = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm.score(X_test, y_test), dummy_majority.score(X_test, y_test)\n",
    "# From the output we can observe that SVC with linear kernel can perform better than the simple dummy classifier \n",
    "# as test score on SVC with linear kernel is much higher than the dummy classifier's test score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Negative class (0) is most frequent class\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "y_majority_predicted = dummy_majority.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_majority_predicted)\n",
    "\n",
    "print('Most frequent class (dummy classifier)\\n', confusion)\n",
    "# Output of the confusion matrix indicates that there is no prediction of positive class\n",
    "# as negative class is predominant in this imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.dummy import DummyClassifier\n",
    "# Two additional methods are being applied for the performance of a dummy classifier\n",
    "# First one corresponds to the class distribution in the training set\n",
    "stratified_dummy = DummyClassifier(strategy = 'stratified', random_state = 0).fit(X_train,y_train)\n",
    "stratified_predicted = stratified_dummy.predict(X_test)\n",
    "confusion  = confusion_matrix(y_test, stratified_predicted)\n",
    "\n",
    "print('Stratified dummy classifier\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second one corresponds to the dummy classifier where dummy randomly classifies with uniform nature\n",
    "uniform_dummy = DummyClassifier(strategy = 'uniform').fit(X_train, y_train)\n",
    "uniform_predicted = uniform_dummy.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, uniform_predicted)\n",
    "\n",
    "print('Uniform strategy for the dummy classifier\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produces random predictions w/ same class proportions as training set\n",
    "dummy_classprop = DummyClassifier(strategy = 'stratified', random_state = 0).fit(X_train, y_train)\n",
    "y_classprop_predicted = dummy_classprop.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, y_classprop_predicted)\n",
    "\n",
    "print('Random class-proportional prediction (dummy classifier)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predicted = svm.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, svm_predicted)\n",
    "\n",
    "print('Support vector machine classifier (linear kernel, C=1)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "lr_predicted = lr.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, lr_predicted)\n",
    "\n",
    "print('Logistic regression classifier (default settings)\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
    "tree_predicted = dt.predict(X_test)\n",
    "confusion = confusion_matrix(y_test, tree_predicted)\n",
    "\n",
    "print('Decision tree classifier (max_depth = 2)\\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "# Accuracy = (TP + TN)/ (TP + TN + FN + FP)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN) Also known as Specificity, or True Positive Rate\n",
    "# F1-score = 2 * Precision * Recall / (Precision + Recall)  \n",
    "\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)) )\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)) )\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)) )\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Combined report with all above metrics\n",
    "print(classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random class-proportional (dummy)\\n',\n",
    "     classification_report(y_test, y_classprop_predicted, target_names = ['not 1', '1']))\n",
    "print('SVM\\n',\n",
    "     classification_report(y_test, svm_predicted, target_names = ['not 1', '1']))\n",
    "print('Logistic regression\\n',\n",
    "     classification_report(y_test, lr_predicted, target_names = ['not 1', '1']))\n",
    "print('Decision tree\\n',\n",
    "     classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n",
    "\n",
    "# It shows decision_function scores for first 20 instances\n",
    "y_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "y_proba_lr = lr.fit(X_train, y_train).predict_proba(X_test)\n",
    "y_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1]))\n",
    "\n",
    "# displays the probability of positive class for first 20 instances\n",
    "y_proba_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "closest_zero = np.argmin(np.abs(thresholds))\n",
    "closest_zero_p = precision[closest_zero]\n",
    "closest_zero_r = recall[closest_zero]\n",
    "closest_zero_r, closest_zero_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlim([0, 1.01])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.plot(precision, recall, label = 'Precision-Recall Curve')\n",
    "plt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c = 'r', mew = 3)\n",
    "plt.xlabel('Precision', fontsize = 16)\n",
    "plt.ylabel('Recall', fontsize = 16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curves, Area-Under-Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "y_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr_lr, tpr_lr, thresholds = roc_curve(y_test, y_scores_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "plt.plot(fpr_lr, tpr_lr, lw = 3, label = 'LogRegr ROC curve (area = {:.2f})'.format(roc_auc_lr))\n",
    "plt.xlabel('False positive rate', fontsize = 16)\n",
    "plt.ylabel('True positive rate', fontsize = 16)\n",
    "plt.title('ROC curve (1-of-10 digits classifier)', fontsize = 16)\n",
    "plt.legend(loc = 'lower right', fontsize = 13)\n",
    "plt.plot([0, 1], [0, 1], color = 'navy', lw = 3, linestyle = '--')\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state = 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlim([-0.01, 1.00])\n",
    "plt.ylim([-0.01, 1.01])\n",
    "\n",
    "for g in [0.01, 0.1, 0.20, 1]:\n",
    "    svm = SVC(gamma = g).fit(X_train, y_train)\n",
    "    y_score_svm = svm.decision_function(X_test)\n",
    "    fpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test, y_score_svm)\n",
    "    roc_auc_svm = auc(fpr_svm, tpr_svm)\n",
    "    accuracy_svm = svm.score(X_test, y_test)\n",
    "    print('gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}'.format(g, accuracy_svm,\n",
    "                                                                  roc_auc_svm))\n",
    "    plt.plot(fpr_svm, tpr_svm, lw = 3, alpha = 0.7,\n",
    "            label = 'SVM (gamma = {:.2f}, area = {:.2f})'.format(g, roc_auc_svm))\n",
    "\n",
    "plt.xlabel('False positive rate', fontsize = 16)\n",
    "plt.ylabel('True positive rate (Recall)', fontsize = 16)\n",
    "plt.plot([0, 1], [0, 1], color = 'k', lw = 0.5, linestyle = '--')\n",
    "plt.legend(loc = 'lower right', fontsize = 11)\n",
    "plt.title('ROC curve: (1-of-10 digits classifier)', fontsize = 16)\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation measures for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_digits()\n",
    "\n",
    "X, y = dataset.data, dataset.target\n",
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_mc = pd.DataFrame(confusion_mc,\n",
    "                    index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n",
    "\n",
    "plt.figure(figsize = (5.5, 4))\n",
    "sns.heatmap(df_mc, annot = True)\n",
    "plt.title('SVM linear kernel \\nAccuracy: {:.3f}'.format(accuracy_score(y_test_mc,\n",
    "                                                                      svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "svm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "confusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\n",
    "df_mc = pd.DataFrame(confusion_mc,\n",
    "                    index = [i for i in range(0, 10)], columns = [i for i in range(0, 10)])\n",
    "\n",
    "plt.figure(figsize = (5.5, 4))\n",
    "sns.heatmap(df_mc, annot = True)\n",
    "plt.title('SVM RBF kernel \\nAccuracy: {:.3f}'.format(accuracy_score(y_test_mc,\n",
    "                                                                   svm_predicted_mc)))\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This classification report corresponds to rbf kernel in SVM classifier\n",
    "# as last svm_predicted_mc denoted rbf kernelized SVM\n",
    "print(classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This classification report for three evaluation metrics\n",
    "# corresponds to SVM classifier with linear kernel\n",
    "svm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "print(classification_report(y_test_mc, svm_predicted_mc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro- vs. Macro-averaged metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These micro- and macro- averaged metrics correspond to linear kernelized SVM classifier\n",
    "print('Micro-averaged precision = {:.2f} (treat instances equally)'.\n",
    "     format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged precision = {:.2f}(treat classes equally)'.\n",
    "     format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n",
    "\n",
    "print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n",
    "     .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n",
    "     .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\n",
    "svm_predicted_mc = svm.predict(X_test_mc)\n",
    "\n",
    "# These micro- and macro- averaged metrics correspond to rbf kernelized SVM classifier\n",
    "print('Micro-averaged precision = {:.2f} (treat instances equally)'.\n",
    "     format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged precision = {:.2f}(treat classes equally)'.\n",
    "     format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))\n",
    "\n",
    "print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n",
    "     .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))\n",
    "print('Macro-averaged f1 = {:.2f} (treat classes equally)'\n",
    "     .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for i in range(len(y_test_mc)):\n",
    "    if y_test_mc[i] == svm_predicted_mc[i]:\n",
    "        matches.append(1)\n",
    "    else:\n",
    "        matches.append(0)\n",
    "micro_averaged_precision = sum(matches)/len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression evaluation metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "X = diabetes.data[:, None, 6]\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "lm = LinearRegression().fit(X_train, y_train)\n",
    "lm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n",
    "\n",
    "y_predict = lm.predict(X_test)\n",
    "y_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n",
    "\n",
    "print('Linear model, coefficients: ', lm.coef_)\n",
    "print('Mean squared error (dummy): {:.2f}'.format(mean_squared_error(y_test,\n",
    "                                                                     y_predict_dummy_mean)))\n",
    "print('Mean squared error (linear mode): {:.2f}'.format(mean_squared_error(y_test,\n",
    "                                                                          y_predict)))\n",
    "print('r2_score (dummy): {:.2f}'.format(r2_score(y_test, y_predict_dummy_mean)))\n",
    "print('r2_score (linear model): {:.2f}'.format(r2_score(y_test, y_predict)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot outputs\n",
    "plt.scatter(X_test, y_test, color = 'black')\n",
    "plt.plot(X_test, y_predict, color = 'green', linewidth = 2,\n",
    "        label = 'linear fit')\n",
    "plt.plot(X_test, y_predict_dummy_mean, color = 'red', linestyle = 'dashed',\n",
    "        linewidth = 2, label = 'dummy')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection using evaluation metrics\n",
    "\n",
    "\n",
    "### Cross-validation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "dataset = load_digits()\n",
    "# Again making this a binary problem with 'digit 1' as positive class\n",
    "# and 'not 1' as negative class\n",
    "\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "clf = SVC(kernel = 'linear', C = 1)\n",
    "\n",
    "# accuracy is the default scoring metric\n",
    "print('Cross-validation (Accuracy)', cross_val_score(clf, X, y, cv = 5))\n",
    "# use AUC as scoring metric\n",
    "print('Cross-validation (AUC)', cross_val_score(clf, X, y, cv = 5, scoring = 'roc_auc'))\n",
    "# use recall as scoring metric\n",
    "print('Cross-validation (recall)', cross_val_score(clf, X, y, cv = 5, scoring = 'recall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "dataset = load_digits()\n",
    "X, y = dataset.data, dataset.target == 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "\n",
    "clf = SVC(kernel = 'rbf')\n",
    "param_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n",
    "\n",
    "\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = param_values)\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test)\n",
    "\n",
    "print('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\n",
    "print('Grid best score (accuracy): ', grid_clf_acc.best_score_)\n",
    "\n",
    "grid_clf_auc = GridSearchCV(clf, param_grid = param_values, scoring = 'roc_auc')\n",
    "grid_clf_auc.fit(X_train, y_train)\n",
    "y_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test)\n",
    "\n",
    "print('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))\n",
    "print('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\n",
    "print('Grid best score (AUC): ', grid_clf_auc.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.scorer import SCORERS\n",
    "print(sorted(list(SCORERS.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
