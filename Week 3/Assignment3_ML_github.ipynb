{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    fraud_data = pd.read_csv('fraud_data.csv')\n",
    "    count_frauds = np.bincount(fraud_data[fraud_data.columns[-1]])[1]\n",
    "    total_obs = fraud_data.shape[0]\n",
    "    fraud_pct = count_frauds/total_obs\n",
    "    return fraud_pct\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score\n",
    "    dum_clf = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n",
    "    acu_dummy = dum_clf.score(X_test, y_test)\n",
    "    dum_pred = dum_clf.predict(X_test)\n",
    "    rec_dummy = recall_score(y_test, dum_pred)    \n",
    "    return acu_dummy, rec_dummy\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "    clf_svm = SVC().fit(X_train, y_train)\n",
    "    svm_predicted = clf_svm.predict(X_test)\n",
    "    svm_acc = clf_svm.score(X_test, y_test)\n",
    "    svm_recall = recall_score(y_test, svm_predicted)\n",
    "    svm_precision = precision_score(y_test, svm_predicted)\n",
    "    svm_acc, svm_recall, svm_precision\n",
    "    return svm_acc, svm_recall, svm_precision\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "    # user defined values for parameters\n",
    "    threshold_value = -220\n",
    "    param_values = {'C': 1e9, 'gamma': 1e-07}\n",
    "    clf_svm2 = SVC(C = param_values['C'], gamma = param_values['gamma']).fit(X_train, y_train)\n",
    "    clf_svm2_scores = clf_svm2.decision_function(X_test)\n",
    "    clf_svm2_predicted = clf_svm2.predict(X_test[clf_svm2_scores > threshold_value])\n",
    "    confusion_svm = confusion_matrix(y_test[clf_svm2_scores > threshold_value], clf_svm2_predicted)\n",
    "    \n",
    "    return confusion_svm\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "    lm = LogisticRegression().fit(X_train, y_train)\n",
    "    lm_predicted = lm.predict(X_test)\n",
    "    lm_probs = lm.predict_proba(X_test)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, lm_probs[:, 1])\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, lm_probs[:, 1])\n",
    "    recall = recall[precision == 0.75][0] \n",
    "    tpr = tpr[np.around(fpr, decimals = 2) == 0.16][0]\n",
    "    return recall, tpr\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    lr_model = LogisticRegression().fit(X_train, y_train) \n",
    "    grid_values = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    grid_lr = GridSearchCV(lr_model, param_grid = grid_values, scoring = 'recall')\n",
    "    grid_lr.fit(X_train, y_train)\n",
    "    mean_test_scores = grid_lr.cv_results_['mean_test_score']\n",
    "    array_test_scores = mean_test_scores.reshape(5,2)\n",
    "    return array_test_scores\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following function to help visualize results from the grid search\n",
    "def GridSearch_Heatmap(scores):\n",
    "    %matplotlib notebook\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "    plt.yticks(rotation=0);\n",
    "\n",
    "GridSearch_Heatmap(answer_six())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
